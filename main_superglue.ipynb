{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-7643 - Deep Learning - Summer 2024 - Final Project - `main.ipynb`\n",
    "# Group - Big Daaata \n",
    "\n",
    "This notebook has code for running experiments to compare training and validation performance and complexity of DistilBERT, BART, and Electra on selected subtasks from the SuperGLUE dataset with and without adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f3cwQ4zBSUog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.local/lib/python3.10/site-packages (from datasets) (0.23.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.local/lib/python3.10/site-packages (from evaluate) (0.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: packaging in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from evaluate) (2.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.10/site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.local/lib/python3.10/site-packages (from evaluate) (2024.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: adapters in ./.local/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: transformers~=4.40.2 in ./.local/lib/python3.10/site-packages (from adapters) (4.40.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (2024.5.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (6.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (4.66.4)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (3.9.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (0.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers~=4.40.2->adapters) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.40.2->adapters) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.40.2->adapters) (2024.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers~=4.40.2->adapters) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers~=4.40.2->adapters) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers~=4.40.2->adapters) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers~=4.40.2->adapters) (1.26.15)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate==0.30 in ./.local/lib/python3.10/site-packages (0.30.0)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.10/site-packages (from accelerate==0.30) (0.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from accelerate==0.30) (23.0)\n",
      "Requirement already satisfied: psutil in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from accelerate==0.30) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from accelerate==0.30) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.10/site-packages (from accelerate==0.30) (0.4.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.10/site-packages (from accelerate==0.30) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from accelerate==0.30) (1.24.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.105)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (3.9.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (3.3)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (2.20.5)\n",
      "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.30) (12.5.82)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30) (4.66.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.30) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install evaluate\n",
    "!pip3 install transformers\n",
    "!pip3 install adapters\n",
    "!pip3 install accelerate==0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RJs70ZZjimj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/maleem3/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding, \n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,      \n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from adapters import (\n",
    "    AdapterArguments,\n",
    "    AdapterTrainer,\n",
    "    AutoAdapterModel,\n",
    "    setup_adapter_training, \n",
    "    AdapterConfig,          \n",
    "\n",
    "    # Pre-defined adapter configurations\n",
    "    BnConfig,\n",
    "    PrefixTuningConfig,\n",
    "    MAMConfig, \n",
    "    CompacterPlusPlusConfig, \n",
    "    UniPELTConfig, \n",
    "    IA3Config,\n",
    "    LoRAConfig, \n",
    "    PromptTuningConfig,\n",
    "    ConfigUnion\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version           \n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-to-Key Mapping\n",
    "\n",
    "The SuperGLUE benchmark consists of 8 tasks. \n",
    "Each task is mapped to a standard key in the underlying `adapters` and `transformers` library code. \n",
    "Each task also has its own input and output. These do not share a static `X` and `y` names. Instead the key is different for each tasks' I/Os.\n",
    "\n",
    "We build a dictionary here called `task_to_keys` that maps each SuperGLUE task to its underlying name in `transformers` along with the names for its I/Os.\n",
    "\n",
    "Also added mapping for GLUE tasks since we decided to explore this benchmark as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"boolq\": (\"question\", \"passage\"),\n",
    "    \"cb\": (\"hypothesis\", \"premise\"),            # Swapped based on Ali's recommendation\n",
    "    \"copa\": (\"premise\", \"choice1\", \"choice2\"),  # TODO: doesn't work - debug\n",
    "    \"multirc\": (\"paragraph\", \"question\"),       # TODO: doesn't work - debug\n",
    "    \"record\": (\"passage\", \"query\"),             # TODO: doesn't work - debug\n",
    "    \"rte\": (\"premise\", \"hypothesis\"),\n",
    "    \"wic\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wsc\": (\"text\", None),\n",
    "\n",
    "    # For glue dataset\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `filter_unused_arguments`\n",
    "\n",
    "Basic function that removes invalid arguments passed through command line, shell, or similar interface. A nicer version of `ArgParse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unused_args(args):\n",
    "    filtered_args = []\n",
    "    for arg in args:\n",
    "        if not arg.startswith(\"-f\") and not (arg.endswith(\".json\") or arg.endswith(\".py\")):\n",
    "            filtered_args.append(arg)\n",
    "    return filtered_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataTrainingArguments`\n",
    "\n",
    "A data validation class for the inputs that get passed to the `datasets `library for instnatiating a `Dataset` object for training with SuperGLUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Basic data-class that defines what arguments can be passed as training data.\n",
    "    Performs basic input validation and post-initialization check.\n",
    "    \"\"\"\n",
    "    task_name: Optional[str] = field(\n",
    "        default='boolq',\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default='super_glue', metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"A csv or a json file containing the test data.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        elif self.dataset_name is not None:\n",
    "            pass\n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE/SuperGLUE task, a training/validation file or a dataset name.\")\n",
    "        else:\n",
    "            train_extension = self.train_file.split(\".\")[-1]\n",
    "            assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            validation_extension = self.validation_file.split(\".\")[-1]\n",
    "            assert (\n",
    "                validation_extension == train_extension\n",
    "            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ModelArguments`\n",
    "\n",
    "A data validation class for the arguments or configurations used to instantiate a `transformers` `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FZ5aXA4acJBk"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        default='distilbert/distilbert-base-uncased',\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    \n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    \n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    \n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizers (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    \n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name, or commit id).\"},\n",
    "    )\n",
    "    \n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    ignore_mismatched_sizes: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Will enable loading a pretrained model whose head dimensions are different.\"},\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TrainingArguments`\n",
    "\n",
    "Simple data validation class for arguments passed to instantiate a `Trainer` using the `transformers `library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    output_dir: str = field(\n",
    "        default=\"./results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"}\n",
    "    )\n",
    "\n",
    "    num_train_epochs: float = field(\n",
    "        default=10.0,\n",
    "        metadata={\"help\": \"Total number of training epochs to perform.\"},\n",
    "    )\n",
    "    \n",
    "    adapter: str = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether you wanna train adapter or fine-tune\"}\n",
    "    )\n",
    "\n",
    "    evaluation_strategy: str = field(\n",
    "        default=\"epoch\",\n",
    "        metadata={\"help\": \"The evaluation strategy to use during training.\"},\n",
    "    )\n",
    "\n",
    "    logging_strategy: str = field(\n",
    "        default=\"epoch\",\n",
    "        metadata={\"help\": \"The logging strategy to use during training.\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Parser\n",
    "\n",
    "We combine all three data validation classes (`Model`, `DataTraining`, `TrainingArguments`) to create a Hugging Face Argument Parser object which accepts, checks, and sanitizes arguments passed through command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "filtered_args = filter_unused_args(sys.argv)\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=filtered_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Helpers\n",
    "\n",
    "Expectation is that this set of generic functions for preprocessing data and computing evaluation metrics will generalize across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(raw_datasets,data_args,model,is_regression,num_labels,label_list,tokenizer):\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "    eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "    predict_dataset = raw_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
    "        predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "\n",
    "    metric = evaluate.load(data_args.dataset_name, data_args.task_name)\n",
    "\n",
    "    data_collator = default_data_collator\n",
    "    \n",
    "    return train_dataset, eval_dataset, predict_dataset, data_args, data_collator, model, metric\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    if data_args.task_name is not None:\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "        if len(result) > 1:\n",
    "            result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,training_args,train_dataset,eval_dataset,tokenizer,data_collator):\n",
    "\n",
    "    trainer_class = AdapterTrainer if training_args.adapter else Trainer\n",
    "    trainer = trainer_class(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    checkpoint = get_last_checkpoint(training_args.output_dir) if training_args.resume_from_checkpoint is None else training_args.resume_from_checkpoint\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "    metrics[\"training_time\"] = training_time\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # Evaluate and log metrics after training\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "    # Initialize list to capture epoch metrics\n",
    "    epoch_metrics_train = []\n",
    "    epoch_metrics_val = []\n",
    "\n",
    "    # Loop through trainer.state.log_history to get metrics at each epoch\n",
    "    for log in trainer.state.log_history:\n",
    "        if 'epoch' in log and 'loss' in log:\n",
    "            epoch_metrics_train.append(log)\n",
    "        if 'epoch' in log and 'eval_loss' in log:\n",
    "            epoch_metrics_val.append(log)\n",
    "\n",
    "    # Create DataFrame from epoch metrics\n",
    "    metrics_df_train = pd.DataFrame(epoch_metrics_train)\n",
    "    metrics_df_val = pd.DataFrame(epoch_metrics_val)\n",
    "    metrics_df = pd.merge(metrics_df_train, metrics_df_val, on='epoch')\n",
    "    metrics_df.rename(columns={'loss': 'train_loss', 'eval_loss': 'val_loss'}, inplace=True)\n",
    "\n",
    "    return trainer, train_result, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(model_string,data_args,training_args):\n",
    "    global sentence1_key, sentence2_key,padding,label_to_id,tokenizer,max_seq_length,is_regression,metric\n",
    "    training_args.adapter_type = model_string\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.task_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=model_args.use_auth_token\n",
    "    )\n",
    "\n",
    "    is_regression = data_args.task_name == \"stsb\"\n",
    "    \n",
    "    if not is_regression:\n",
    "        label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=data_args.task_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=model_args.use_auth_token,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        use_auth_token=model_args.use_auth_token\n",
    "    )\n",
    "    if training_args.adapter:\n",
    "        model = AutoAdapterModel.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=model_args.use_auth_token,\n",
    "            ignore_mismatched_sizes=model_args.ignore_mismatched_sizes\n",
    "        )\n",
    "        model.add_classification_head(\n",
    "        data_args.dataset_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={i: v for i, v in enumerate(label_list)} if not is_regression else None,\n",
    "        )\n",
    "        adapter_config_kwargs = {}\n",
    "        adapter_load_kwargs = {}\n",
    "        model.add_adapter(training_args.adapter_type, config=string_to_config[training_args.adapter_type])\n",
    "        if training_args.adapter_type=='prefix_tuning':\n",
    "            model.eject_prefix_tuning(\"prefix_tuning\")\n",
    "        model.train_adapter([training_args.adapter_type])\n",
    "        model.set_active_adapters(training_args.adapter_type)\n",
    "    else:\n",
    "        model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=model_args.use_auth_token,\n",
    "            ignore_mismatched_sizes=model_args.ignore_mismatched_sizes\n",
    "        )\n",
    "    sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "    label_to_id = None\n",
    "    if (\n",
    "        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "        and data_args.task_name is not None\n",
    "        and not is_regression\n",
    "    ):\n",
    "        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "                f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "                \"\\nIgnoring the model labels as a result.\",\n",
    "            )\n",
    "    elif data_args.task_name is None and not is_regression:\n",
    "        label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "    elif data_args.task_name is not None and not is_regression:\n",
    "        model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    train_dataset, eval_dataset, predict_dataset, data_args, data_collator, model, metric = \\\n",
    "                            preprocess_data(raw_datasets,data_args,model,is_regression,num_labels,label_list,tokenizer)\n",
    "    trainer, train_result = run_model(model,training_args,train_dataset,eval_dataset,tokenizer,data_collator)\n",
    "    \n",
    "    eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    eval_accuracy = eval_metrics['eval_accuracy']\n",
    "    \n",
    "    return eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_results_summary(results_dict, user_name = 'saad', output_dir = \"./\"):\n",
    "  \"\"\"\n",
    "  Helper function that accepts a dictionary of results returned by the `run_iteration`\n",
    "  function and extracts all relevant information into a dataframe.\n",
    "\n",
    "  Expects results_dict will be a dictionary of dictionaries with key = model name\n",
    "  and values = the result set of a single run of the `run_iteration` function.\n",
    "\n",
    "  Args\n",
    "  - `results_dict` (dict): Dicitonary of dictionaries returned from `run_iteration`\n",
    "  - `user_name` (str): Name of the user who generated the results\n",
    "  \"\"\"\n",
    "  all_results_df = None\n",
    "\n",
    "  for idx, curr_results_dict in enumerate(results_dict.values()):\n",
    "    metadata_df = pd.DataFrame({\n",
    "        'model_name':  curr_results_dict['model_string'],\n",
    "        'dataset_name': curr_results_dict['dataset_name'],\n",
    "        'task_name': curr_results_dict['task_name'],\n",
    "        'used_adapter': curr_results_dict['used_adapter'],\n",
    "        'adapter_type': curr_results_dict['adapter_type']\n",
    "    }, index = [idx])\n",
    "\n",
    "    train_metrics_df = pd.DataFrame(curr_results_dict['train_result'].metrics, index = [idx])\n",
    "    eval_metrics_df = pd.DataFrame(curr_results_dict['eval_metrics'], index = [idx])\n",
    "    curr_results_df = pd.concat([metadata_df, train_metrics_df, eval_metrics_df], axis = 1)\n",
    "\n",
    "    if all_results_df is None:\n",
    "      all_results_df = curr_results_df\n",
    "    else:\n",
    "      all_results_df = pd.concat([all_results_df, curr_results_df], axis = 0)\n",
    "\n",
    "  all_results_df_cols_orig = all_results_df.columns.tolist()\n",
    "  all_results_df['user'] = user_name\n",
    "  all_results_df['generated_on'] = str(datetime.datetime.now())\n",
    "  all_results_df_final = ['user', 'generated_on'] + all_results_df_cols_orig\n",
    "  all_results_df = all_results_df[all_results_df_final]\n",
    "\n",
    "  str_model_name = metadata_df['model_name'].unique()[0]\n",
    "  str_model_name = [char for char in str_model_name if char not in ['/', '\\\\']]\n",
    "  str_model_name = ''.join(str_model_name)\n",
    "  str_dataset_name = metadata_df['dataset_name'].unique()[0]\n",
    "  str_task_name = metadata_df['task_name'].unique()[0]\n",
    "  str_adapter_name = metadata_df['used_adapter'].unique()[0]\n",
    "\n",
    "  file_path_str = f'{output_dir}/summary_{str_dataset_name}_{str_task_name}_{str_model_name}_{str_adapter_name}_{str(datetime.datetime.now())}.csv'\n",
    "  print(f\"Attempting to save file to {file_path_str}\")\n",
    "\n",
    "  all_results_df.to_csv(file_path_str)\n",
    "\n",
    "  return all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_results_epoch_summary(histories_dict, user_name='saad', cols_to_get = ['train_loss', 'val_loss'], output_dir=\"./\"):\n",
    "  all_epochs_results_df = None\n",
    "  # meta_data_cols = ['model_string', 'dataset_name', 'task_name', 'used_adapter', 'adapter_type', 'epoch']\n",
    "  # metric_cols = ['train_loss', 'val_loss']\n",
    "  # all_cols = meta_data_cols + metric_cols\n",
    "  for idx, curr_histories_df in enumerate(histories_dict.values()):\n",
    "    curr_epochs_results_df = curr_histories_df\n",
    "\n",
    "    if all_epochs_results_df is None:\n",
    "      all_epochs_results_df = curr_epochs_results_df\n",
    "    else:\n",
    "      all_epochs_results_df = pd.concat([all_epochs_results_df, curr_epochs_results_df], axis = 0)\n",
    "\n",
    "  all_epochs_results_df_cols_orig = all_epochs_results_df.columns.tolist()\n",
    "  all_epochs_results_df['user_name'] = user_name\n",
    "  all_epochs_results_df['generated_on'] = str(datetime.datetime.now())\n",
    "  all_epochs_results_df_cols_final = ['user_name', 'generated_on'] + all_epochs_results_df_cols_orig\n",
    "  all_epochs_results_df_final = all_epochs_results_df[all_epochs_results_df_cols_final]\n",
    "  \n",
    "  str_model_name = all_epochs_results_df['model_string'].unique()[0]\n",
    "  str_model_name = [char for char in str_model_name if char not in ['/', '\\\\']]\n",
    "  str_model_name = ''.join(str_model_name)\n",
    "  str_dataset_name = all_epochs_results_df['dataset_name'].unique()[0]\n",
    "  str_task_name = all_epochs_results_df['task_name'].unique()[0]\n",
    "  str_adapter_name = all_epochs_results_df['used_adapter'].unique()[0]\n",
    "  file_path_str = f'{output_dir}/epochs_{str_dataset_name}_{str_task_name}_{str_model_name}_{str_adapter_name}_{str(datetime.datetime.now())}.csv'\n",
    "  all_epochs_results_df_final.to_csv(file_path_str) \n",
    "\n",
    "  return all_epochs_results_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_learning_curves(learning_curves_df, fig_size = (8, 8), output_dir = \"./\"):\n",
    "  dataset_name = learning_curves_df['dataset_name'].unique()[0]\n",
    "  task_name = learning_curves_df['task_name'].unique()[0]\n",
    "  model_name = learning_curves_df['model_string'].unique()[0]\n",
    "\n",
    "  metadata_cols = ['user_name', 'generated_on', 'model_string', 'dataset_name', 'task_name', 'used_adapter', 'adapter_type', 'epoch']\n",
    "  metric_cols = ['train_loss', 'val_loss']\n",
    "  learning_curves_df = learning_curves_df[metadata_cols + metric_cols]\n",
    "   \n",
    "  learning_curves_df_long = learning_curves_df.melt(\n",
    "      id_vars = ['user_name', 'generated_on', 'model_string', 'dataset_name', 'task_name', 'used_adapter', 'adapter_type', 'epoch'],\n",
    "      var_name = 'metric', value_name = 'value')\n",
    "  learning_curves_df_plot = (\n",
    "      pn.ggplot(learning_curves_df_long, pn.aes(x = 'epoch', y = 'value', color = 'metric')) +\n",
    "      pn.geom_line() +\n",
    "      pn.geom_point() +\n",
    "      pn.facet_wrap('~  adapter_type', ncol = 3) +\n",
    "      pn.labs(title = f'Learning Curves - {dataset_name} - {task_name} - {model_name}', x = 'Epoch', y = 'Loss') +\n",
    "      pn.theme(figure_size = fig_size)\n",
    "  )\n",
    "  str_model_name = learning_curves_df['model_string'].unique()[0]\n",
    "  str_model_name = [char for char in str_model_name if char not in ['/', '\\\\']]\n",
    "  str_model_name = ''.join(str_model_name)\n",
    "  str_dataset_name = learning_curves_df['dataset_name'].unique()[0]\n",
    "  str_task_name = learning_curves_df['task_name'].unique()[0]\n",
    "  str_adapter_name = learning_curves_df['used_adapter'].unique()[0]\n",
    "\n",
    "  file_path_str = f\"{output_dir}/learning_curve_{str_dataset_name}_{str_task_name}_{str_model_name}_{str_adapter_name}_{str(datetime.datetime.now())}.png\"\n",
    "  learning_curves_df_plot.save(file_path_str, width = fig_size[0], height = fig_size[1])\n",
    "  print(learning_curves_df_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter Configuration Mapping\n",
    "\n",
    "The `adapters` library provides several versions of adapter objects that can be added to any transformer-based architecture. \n",
    "\n",
    "Here, we build a mapping between these adapters and names that we can use to access pre-instantiated adapters with reasonable defaults for configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_config = {\n",
    "    'seq_bn': BnConfig(mh_adapter=True, output_adapter=True, reduction_factor=16, non_linearity=\"relu\"),\n",
    "    \"prefix_tuning\": PrefixTuningConfig(flat=False, prefix_length=30),\n",
    "    \"mam_adapter\": MAMConfig(),\n",
    "    \"compacter_plusplus\": CompacterPlusPlusConfig(),\n",
    "    \"unipelt\":UniPELTConfig(),\n",
    "    \"lora\": LoRAConfig(r=8, alpha=16),\n",
    "    \"seq_bn_16_2_relu\": ConfigUnion(\n",
    "        BnConfig(mh_adapter=True, output_adapter=False, reduction_factor=16, non_linearity=\"relu\"),\n",
    "        BnConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity=\"relu\"),\n",
    "    ),\n",
    "    \"seq_bn_16_2_tanh\": ConfigUnion(\n",
    "        BnConfig(mh_adapter=True, output_adapter=False, reduction_factor=16, non_linearity=\"tanh\"),\n",
    "        BnConfig(mh_adapter=False, output_adapter=True, reduction_factor=2, non_linearity=\"tanh\"),\n",
    "    ),\n",
    "    \"ia3\":IA3Config(),\n",
    "    'prompt_tuning': PromptTuningConfig(prompt_length=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "For our project, an experiment consists of \n",
    "1. choosing a task from the SuperGLUE dataset\n",
    "2. choosing one of three transformer models\n",
    "3. configuring whether or not to use an adapter\n",
    "4. iterating over all specified adapters in `string_to_config` mapping\n",
    "5. recording results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Hyperparams Here\n",
    "# valid_model_names = ['distilbert/distilbert-base-uncased', 'google/electra-large-discriminator', 'facebook/bart-base'] \n",
    "model_args.model_name_or_path = 'facebook/bart-base'\n",
    "data_args.dataset_name = 'super_glue'\n",
    "data_args.task_name = 'cb'\n",
    "training_args.adapter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = dict()\n",
    "histories = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU cannot allocate memory, reset session -> add any previously executed\n",
    "# adapter names here\n",
    "models_to_exclude = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_string in string_to_config.keys():\n",
    "    if training_args.adapter:\n",
    "      if model_string in models_to_exclude:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Running with model_string: {model_string}\")\n",
    "        accuracies[model_string], histories[model_string] = run_iteration(model_string,data_args,training_args)\n",
    "    else:\n",
    "      if model_string != 'seq_bn':\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Running with dummy model_string: {model_string} and adapter False\")\n",
    "        accuracies[model_string], histories[model_string] = run_iteration(model_string,data_args,training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results_df = get_experiment_results_summary(accuracies, output_dir=\"/content/\")\n",
    "my_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_histories_df = get_experiment_results_epoch_summary(histories, output_dir=\"/content/\")\n",
    "my_histories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_learning_curves(my_histories_df, fig_size=(12, 12), output_dir=\"/content/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files_output_dir = \"/content/\"\n",
    "files_to_download = os.listdir(files_output_dir)\n",
    "files_to_download = [fname for fname in files_to_download if fname.endswith(\".csv\") or fname.endswith(\".png\")]\n",
    "for fname in files_to_download:\n",
    "  files.download(fname)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
