{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install evaluate\n",
    "!pip3 install transformers\n",
    "!pip3 install adapters\n",
    "!pip3 install accelerate==0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from adapters import AdapterArguments, AdapterTrainer, AutoAdapterModel, setup_adapter_training, AdapterConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"boolq\": (\"question\", \"passage\"), #works\n",
    "    \"cb\": (\"hypothesis\", \"premise\"), #works (the tags should be swapped)\n",
    "    \"copa\": (\"premise\", \"choice1\", \"choice2\"), #nope\n",
    "    \"multirc\": (\"paragraph\", \"question\"), #nope\n",
    "    \"record\": (\"passage\", \"query\"), #nope\n",
    "    \"rte\": (\"premise\", \"hypothesis\"),#works\n",
    "    \"wic\": (\"sentence1\", \"sentence2\"), #nope\n",
    "    \"wsc\": (\"text\", None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolq = load_dataset(\"super_glue\", \"boolq\")\n",
    "# cb = load_dataset(\"super_glue\", \"cb\")\n",
    "# copa = load_dataset(\"super_glue\", \"copa\")\n",
    "# multirc = load_dataset(\"super_glue\", \"multirc\")\n",
    "# record = load_dataset(\"super_glue\", \"record\")\n",
    "# rte = load_dataset(\"super_glue\", \"rte\")\n",
    "# wic = load_dataset(\"super_glue\", \"wic\")\n",
    "# wsc = load_dataset(\"super_glue\", \"wsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': 'While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department\\'s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government\\'s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on  Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President\\'s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed  Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding  Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he\\'d help us get  Bin Laden and deal with another issue or two.\" The U.S. effort continued. ',\n",
       " 'question': 'What did the high-level effort to persuade Pakistan include?',\n",
       " 'answer': 'Children, Gerd, or Dorian Popa',\n",
       " 'idx': {'paragraph': 0, 'question': 0, 'answer': 0},\n",
       " 'label': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boolq[\"train\"][0] #question : passage : label \n",
    "# cb[\"train\"][0] #premise : hypothesis: label\n",
    "# copa[\"train\"][1]\n",
    "# multirc[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unused_args(args):\n",
    "    filtered_args = []\n",
    "    for arg in args:\n",
    "        if not arg.startswith(\"-f\") and not (arg.endswith(\".json\") or arg.endswith(\".py\")):\n",
    "            filtered_args.append(arg)\n",
    "    return filtered_args\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    task_name: Optional[str] = field(\n",
    "        default='boolq',\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default='super_glue', metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        elif self.dataset_name is not None:\n",
    "            pass\n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE task, a training/validation file or a dataset name.\")\n",
    "        else:\n",
    "            train_extension = self.train_file.split(\".\")[-1]\n",
    "            assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            validation_extension = self.validation_file.split(\".\")[-1]\n",
    "            assert (\n",
    "                validation_extension == train_extension\n",
    "            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        default='distilbert/distilbert-base-uncased',\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizers (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name, or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_mismatched_sizes: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Will enable loading a pretrained model whose head dimensions are different.\"},\n",
    "    )\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    output_dir: str = field(\n",
    "        default=\"./results\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"}\n",
    "    )\n",
    "    adapter: str = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether you wanna train adapter or fine-tune\"}\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "filtered_args = filter_unused_args(sys.argv)\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=filtered_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Hyperparams Here\n",
    "model_args.model_name_or_path = 'distilbert/distilbert-base-uncased'\n",
    "data_args.dataset_name = 'super_glue'\n",
    "data_args.task_name = 'record'\n",
    "training_args.adapter = False\n",
    "training_args.adapter_type = \"seq_bn\"\n",
    "training_args.save_strategy = 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/msheikh36/.local/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=model_args.use_auth_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01312dc6b254659bed72caf048e8360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/1179400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999b64ded2e64847a73a1253c2c28377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1179400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b58daa6301048c2bfefd5a11a816ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/113236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a107e9360bf0492990b32f8237435789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/113236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if data_args.task_name == \"record\":\n",
    "    from datasets import Dataset, DatasetDict\n",
    "    import re\n",
    "\n",
    "    def preprocess_dataset(data):\n",
    "        passages, queries, labels, idxs, entities = [], [], [], [], []\n",
    "        \n",
    "        for row in data:\n",
    "            passage_text = row['passage']\n",
    "            query_text = row['query']\n",
    "            answer = row['answers'][0]\n",
    "            entities_list = row['entities']\n",
    "            idx_ = row['idx']\n",
    "\n",
    "            for entity in entities_list:\n",
    "                passages.append(passage_text)\n",
    "                cleaned_entity = re.sub(r\"\\\\\", \"\", entity)\n",
    "                queries.append(re.sub(\"@placeholder\", cleaned_entity, query_text))\n",
    "                labels.append(answer == cleaned_entity)\n",
    "                idxs.append(idx_)\n",
    "                entities.append(entity)\n",
    "\n",
    "        dataset = Dataset.from_dict({\n",
    "            \"passage\": passages,\n",
    "            \"query\": queries,\n",
    "            \"label\": labels,\n",
    "            \"idx\": idxs\n",
    "        })\n",
    "        return dataset.class_encode_column(\"label\")\n",
    "\n",
    "    train_data = preprocess_dataset(raw_datasets[\"train\"])\n",
    "    validation_data = preprocess_dataset(raw_datasets[\"validation\"])\n",
    "\n",
    "    raw_datasets = DatasetDict({'train': train_data, 'validation': validation_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['False', 'True']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/msheikh36/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:913: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/msheikh36/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hice1/msheikh36/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/msheikh36/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "is_regression = data_args.task_name == \"stsb\"\n",
    "if not is_regression:\n",
    "    label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "    num_labels = len(label_list)\n",
    "    print(label_list)\n",
    "else:\n",
    "    num_labels = 1\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=model_args.use_auth_token,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    use_auth_token=model_args.use_auth_token\n",
    ")\n",
    "\n",
    "if training_args.adapter:\n",
    "    model = AutoAdapterModel.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=model_args.use_auth_token,\n",
    "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes\n",
    "    )\n",
    "    model.add_classification_head(\n",
    "    data_args.dataset_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if not is_regression else None,\n",
    "    )\n",
    "    adapter_config_kwargs = {}\n",
    "    adapter_load_kwargs = {}\n",
    "    adapter_config = AdapterConfig.load(training_args.adapter_type, **adapter_config_kwargs)\n",
    "    model.add_adapter(data_args.task_name, config=adapter_config)\n",
    "    model.train_adapter([data_args.task_name])\n",
    "    model.set_active_adapters(data_args.task_name)\n",
    "else:\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=model_args.use_auth_token,\n",
    "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id and data_args.task_name is not None and not is_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and data_args.task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    print(f\"pretrainconfig labels mapping : {PretrainedConfig(num_labels=num_labels).label2id}, model label mappings : {model.config.label2id}\")\n",
    "    label_name_to_id = {k: v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif data_args.task_name is None and not is_regression:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "elif data_args.task_name is not None and not is_regression:\n",
    "    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda679abd7724ae786171d42ef9134ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1179400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0890a844e4532bf79069392620f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/113236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if data_args.max_train_samples is not None:\n",
    "    max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
    "if data_args.max_eval_samples is not None:\n",
    "    max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "# predict_dataset = raw_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
    "# if data_args.max_predict_samples is not None:\n",
    "#     max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
    "#     predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "\n",
    "# metric = evaluate.load(\"super_glue\", data_args.task_name)\n",
    "\n",
    "# def compute_metrics(p: EvalPrediction):\n",
    "#     preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "#     preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "#     if data_args.task_name is not None:\n",
    "#         result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "#         if len(result) > 1:\n",
    "#             result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "#         return result\n",
    "#     elif is_regression:\n",
    "#         return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "#     else:\n",
    "#         return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "# data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "metric = evaluate.load(\"super_glue\", data_args.task_name)\n",
    "def compute_metrics(p):\n",
    "    if data_args.task_name == \"record\":\n",
    "        preds = p.predictions\n",
    "        labels = p.label_ids\n",
    "\n",
    "        print(f\"Predictions shape: {preds.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "        if preds.ndim == 2:  # This means we have logits\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average='weighted')  # Adjust 'average' as needed\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    else : \n",
    "        def compute_metrics(p: EvalPrediction):\n",
    "            preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "            preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "            if data_args.task_name is not None:\n",
    "                result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "                if len(result) > 1:\n",
    "                    result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "                return result\n",
    "            elif is_regression:\n",
    "                return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "            else:\n",
    "                return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_class = AdapterTrainer if training_args.adapter else Trainer\n",
    "print(trainer_class)\n",
    "trainer = trainer_class(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "checkpoint = get_last_checkpoint(training_args.output_dir) if training_args.resume_from_checkpoint is None else training_args.resume_from_checkpoint\n",
    "train_result = trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "metrics[\"training_time\"] = training_time\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "print(\"Training metrics:\", metrics)\n",
    "\n",
    "eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "if 'accuracy' in eval_metrics:\n",
    "    accuracy = eval_metrics['accuracy']\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
